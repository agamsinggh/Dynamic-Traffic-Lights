# -*- coding: utf-8 -*-
"""traffic congestion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AGtqv776rdF9YidZFDHDZqQmTi9P2OyF
"""

!pip install opendatasets --upgrade --quiet



import opendatasets as od

dataset_url = 'https://www.kaggle.com/datasets/farzadnekouei/top-view-vehicle-detection-image-dataset'
od.download(dataset_url)

!pip install ultralytics

# Disable warnings in the notebook to maintain clean output cells
import warnings
warnings.filterwarnings('ignore')

# Import necessary libraries
import os
import shutil
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import yaml
import torch
import ultralytics
from PIL import Image
from ultralytics import YOLO
from IPython.display import Video
from collections import Counter
import random

print('Currently using GPU' if torch.cuda.is_available() else 'Currently using CPU')

File_Data_Path = '/content/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset'
Train_Data_Path = '/content/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/train'
Valid_Data_Path=  '/content/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/valid'
Image_Data_Path = '/content/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_image.jpg'
Video_Data_Path = '/content/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_video.mp4'

def count_class_ids_in_yolo_files(directory_path):
    class_id_counts = Counter()
    for filename in os.listdir(directory_path):
        if filename.endswith('.txt'):
            file_path = os.path.join(directory_path, filename)
            with open(file_path, 'r') as file:
                for line in file:
                    class_id = line.split()[0]
                    class_id_counts[class_id] += 1

    return class_id_counts
directory_path = "/content/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/train/labels"
class_id_counts = count_class_ids_in_yolo_files(directory_path)

for class_id, count in class_id_counts.items():
    print(f"Class ID {class_id} (vehicle) : {count} samples")

# Load a pretrained YOLOv8n model from Ultralytics
model = YOLO('yolo11l.pt')

def Show_Predicted_Images(model , source , imgsz , conf , title ,figsize ):
    prediction = model.predict(source=source , imgsz = imgsz ,conf = conf )
    img = prediction[0].plot(line_width = 2)
    img = cv2.cvtColor(img , cv2.COLOR_BGR2RGB)
    plt.figure(figsize=figsize)
    plt.imshow(img)
    plt.title(title, fontsize=20)
    plt.axis('off')
    plt.show()

Train_images = []
Valid_images = []
Train_images_mode = []
Valid_images_mode = []
for img in os.listdir(Train_Data_Path+'/images'):
            Train_images.append( os.path.join(Train_Data_Path+'/images',img))
            Train_images_mode.append(img.split('.')[3])
for img in os.listdir(Valid_Data_Path+'/images'):
            Valid_images.append(os.path.join(Valid_Data_Path+'/images',img))
            Valid_images_mode.append(img.split('.')[3])

print(f'The number of Train images : {len(Train_images)}')
print(f'The number of valid images : {len(Valid_images)}')
print('--'* 20 )
print(f'The Mode of all Train images : {pd.Series(Train_images_mode).value_counts()}')
print('--'* 20 )
print(f'The Mode of all Valid images : {pd.Series(Valid_images_mode).value_counts()}')

def show_images_train(path , num , seed=42 , title=None):
    plt.style.use('dark_background')
    random.seed(seed)
    number_image = random.choices( path,k=num)
    plt.figure(figsize=(15 , 12))
    plt.suptitle(title , fontsize=15 )
    for i, images in enumerate(number_image):
          plt.subplot(3,4,i+1)
          img = Image.open(images)
          plt.title(f'size : ({img.width} ,{img.height} )')
          plt.imshow(img)

show_images_train(path = Train_images , num=12 , title='Train images')

def show_images_predicted(path , num , Model=model ,Conf=0.5 , seed=42 , title=None):
    plt.style.use('dark_background')
    random.seed(seed)
    number_image = random.choices( path,k=num)
    plt.figure(figsize=(15 , 12))
    plt.suptitle(title , fontsize=15 ,fontstyle='italic' , c='w' )
    for inx, images in enumerate(number_image):
          plt.subplot(3,4,inx+1)
          prediction = Model.predict(source=images , imgsz=640 , conf=Conf)
          predic_image = prediction[0].plot(line_width=2)
          color_channel = cv2.cvtColor(predic_image , cv2.COLOR_BGR2RGB)
          plt.imshow(color_channel)

show_images_predicted(path=Train_images , num=12 , title='Prediction By YOLOv11L Pretrained ' )

def show_images(name, figsize , suptitle ):
    plt.figure(figsize=figsize )
    plt.suptitle(suptitle)
    imgs = plt.imread(name)
    plt.imshow(imgs)
    plt.axis('off')
    plt.show()
    print(f'The shape of the image is : {imgs.shape}')
    formt = name.split('.')[1]
    print(f'The format of the image is :{formt}')

show_images(Image_Data_Path , (10,5) , 'Sample Image')

Video(Video_Data_Path, embed=True, width=900)

yaml_file_data_path='/content/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/data.yaml'
with open(yaml_file_data_path , 'r') as file:
    yaml_content= yaml.load(file, Loader=yaml.FullLoader)
print(yaml.dump(yaml_content,default_flow_style=False))

History=model.train(
    data=yaml_file_data_path,
    epochs=150,
    imgsz=640,
    device=0,
    patience=20,
    batch=16,
    optimizer='auto',
    lr0=0.001,
    lrf=0.01,
    dropout=0.1,
    augment= True,
    mosaic=True,
    mixup=False,
    seed=42
)

results_path = "/content/runs/detect/train"

def plot_yolo_training_results(run_folder_path , columns , title):

    csv_path = os.path.join(run_folder_path, 'results.csv')

    if not os.path.exists(csv_path):
        print(f"Error: {csv_path} does not exist.")
        return

    data = pd.read_csv(csv_path)
    if data.empty:
        print("Error: The results.csv file is empty.")
        return
    epochs = data.index
    plt.figure(figsize=(10, 6))

    for column in data.columns:
        if column in columns:
            plt.plot(epochs, data[column],'--',label=column  )

    plt.title(f'{title}')
    plt.xlabel("Epochs")
    plt.ylabel("Metrics")
    plt.legend()
    plt.grid(True)
    plt.show()

plot_yolo_training_results(results_path , columns=['metrics/mAP50-95(B)' ,'metrics/mAP50(B)','metrics/recall(B)' ] , title='Train Accuracy Metrics' )

plot_yolo_training_results(results_path , columns=['train/box_loss' ,'train/dfl_loss' , "train/cls_loss" ] , title='Train Loss Metrics')

df = pd.read_csv(results_path + '/results.csv')
df

def show_confusion_matrix_images(image_path1, image_path2, title1, title2):
    img1 = Image.open(image_path1)
    img2 = Image.open(image_path2)
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    axes[0].imshow(img1)
    axes[0].set_title(title1, fontsize=14)
    axes[0].axis("off")
    axes[1].imshow(img2)
    axes[1].set_title(title2, fontsize=14)
    axes[1].axis("off")
    plt.tight_layout()
    plt.show()
image_path1 = results_path + "/confusion_matrix.png"
image_path2 = results_path + "/confusion_matrix_normalized.png"
show_confusion_matrix_images(image_path1, image_path2 , title1='Confusion Matrix' , title2='Confusion Matrix_Normalized')

def show_PR_curves(img_path1, img_path2, img_path3, img_path4, titles=None):
    image_paths = [img_path1, img_path2, img_path3, img_path4]

    fig, axes = plt.subplots(2, 2, figsize=(10, 10))
    for i, ax in enumerate(axes.flatten()):
        img = Image.open(image_paths[i])
        ax.imshow(img)
        if titles and i < len(titles):
            ax.set_title(titles[i], fontsize=14)
        ax.axis("off")
    plt.tight_layout()
    plt.show()

show_PR_curves(
    results_path+'/R_curve.png',
    results_path+"/P_curve.png",
    results_path+"/PR_curve.png",
    results_path+"/F1_curve.png",
    titles=["R_curve", "P_curve", "PR_curve", "F1_curve"]
)

Best_Model_Path = results_path+'/weights/best.pt'
Best_Model= YOLO(Best_Model_Path )

show_images_predicted(path=Train_images , num=12 ,Model=Best_Model ,title='Prediction By YOLOv11l Trained on Custom dataset ' )

results = Best_Model.predict(source=Image_Data_Path, imgsz=640, conf=0.4)
sample_image = results[0].plot(line_width=2)
sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)
plt.figure(figsize=(15 , 10))
plt.imshow(sample_image)
plt.title('Detected Objects in Sample Image by the Fine-tuned YOLOv11L Model', fontsize=20)
plt.axis('off')
plt.show()

video_path ="/content/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_video.mp4"
Best_Model.predict(source=video_path, save=True ,conf=0.25 )

import subprocess
subprocess.run(['ffmpeg', '-y', '-loglevel', 'panic', '-i', "/content/runs/detect/predict/sample_video.avi", 'result.mp4'])

Video("/content/result.mp4" , width=700 , height=500 , embed=True)

from collections import defaultdict,deque
import time

import cv2
import time
import numpy as np
from collections import defaultdict
from ultralytics import YOLO  # Assuming YOLO is imported from the correct package

def get_color_for_id(track_id):
    np.random.seed(track_id)
    return tuple(np.random.randint(0, 255, size=3).tolist())

def draw_text_with_background(image, text, position, font=cv2.FONT_HERSHEY_SIMPLEX,
                              font_scale=1, font_thickness=2, text_color=(255, 255, 255), bg_color=(0, 0, 0), padding=5):
    text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]
    text_width, text_height = text_size
    x, y = position
    top_left = (x, y - text_height - padding)
    bottom_right = (x + text_width + padding * 2, y + padding)
    cv2.rectangle(image, top_left, bottom_right, bg_color, -1)
    cv2.putText(image, text, (x + padding, y), font, font_scale, text_color, font_thickness, cv2.LINE_AA)

class YOLOVideoProcessor:
    def __init__(self, model_path, video_path, output_path, classes=[0], tracker_method="bot"):
        self.model = YOLO(model_path, task="detect")
        self.classes = classes
        self.tracker_method = tracker_method
        self.track_colors = {}
        self.track_history = defaultdict(list)
        self.vehicle_region1_history = defaultdict(int)  # Track how long each vehicle stays in Region 1
        self.vehicle_region2_history = defaultdict(int)  # Track how long each vehicle stays in Region 2
        self.video_path = video_path
        self.output_path = output_path
        self.region1_active_ids = set()
        self.region2_active_ids = set()

    def is_in_region(self, center, poly):
        poly_np = np.array(poly, dtype=np.int32)
        return cv2.pointPolygonTest(poly_np, center, False) >= 0

    def process_video(self):
        cap = cv2.VideoCapture(self.video_path)
        prev_frame_time = 0
        all_cars = 0
        region1_cars = 0
        region2_cars = 0
        region1_status = "smooth"
        region2_status = "smooth"

        poly1 = [(465, 350), (609, 350), (520, 630), (3, 630)]  # Region 1 polygon
        poly2 = [(678, 350), (815, 350), (1203, 630), (743, 630)]  # Region 2 polygon

        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(self.output_path, fourcc, 30, (frame_width, frame_height))

        while cap.isOpened():
            success, frame = cap.read()
            if not success:
                break

            tracker = "botsort.yaml" if self.tracker_method.lower() == "bot" else "bytetrack.yaml"

            results = self.model.track(frame, persist=True, tracker=tracker, classes=self.classes, conf=0.25)

            if len(results[0].boxes) == 0:
                boxes = []
                track_ids = []
                confidences = []
            else:
                boxes = results[0].boxes.xywh.cpu().numpy()
                confidences = results[0].boxes.conf.cpu().numpy()

                try:
                    track_ids = results[0].boxes.id.int().cpu().tolist()
                except AttributeError:
                    print("Tracking IDs not found.")
                    track_ids = [i for i in range(len(boxes))]

            annotated_frame = frame.copy()
            cv2.polylines(annotated_frame, [np.array(poly1, dtype=np.int32)], isClosed=True, color=(255, 0, 0), thickness=2)
            cv2.polylines(annotated_frame, [np.array(poly2, dtype=np.int32)], isClosed=True, color=(0, 255, 0), thickness=2)

            current_region1_ids = set()
            current_region2_ids = set()
            if len(boxes) > 0 and len(track_ids) > 0:
                for box, track_id, confidence in zip(boxes, track_ids, confidences):
                    x, y, w, h = box

                    color = get_color_for_id(track_id)
                    cv2.rectangle(annotated_frame, (int(x - w / 2), int(y - h / 2)),
                                  (int(x + w / 2), int(y + h / 2)), color, 2)
                    draw_text_with_background(annotated_frame, f'ID: {track_id} ({confidence:.2f})',
                                              (int(x - w / 2), int(y - h / 2) - 10), bg_color=color)

                    center_point = (int(x), int(y))
                    cv2.circle(annotated_frame, center_point, radius=4, color=color, thickness=-1)
                    center = (float(x), float(y))
                    self.track_history[track_id].append(center)

                    if len(self.track_history[track_id]) > 15:
                        self.track_history[track_id].pop(0)

                    points = np.array(self.track_history[track_id]).astype(np.int32).reshape((-1, 1, 2))
                    cv2.polylines(annotated_frame, [points], isClosed=False, color=color, thickness=2)

                    if self.is_in_region(center_point, poly1):
                        current_region1_ids.add(track_id)
                        self.vehicle_region1_history[track_id] += 1  # Track how long the vehicle stays in Region 1

                    if self.is_in_region(center_point, poly2):
                        current_region2_ids.add(track_id)
                        self.vehicle_region2_history[track_id] += 1  # Track how long the vehicle stays in Region 2

            # Use a threshold for vehicle stay duration in the region to filter out short-term fluctuations
            region1_cars = sum(1 for car in current_region1_ids if self.vehicle_region1_history[car] > 5)
            region2_cars = sum(1 for car in current_region2_ids if self.vehicle_region2_history[car] > 5)

            # Debugging: Print the number of cars in each region
            print(f"Region 1 cars: {region1_cars}, Region 2 cars: {region2_cars}")

            # Set region status
            region1_status = "heavy" if region1_cars > region2_cars else "smooth"
            region2_status = "heavy" if region2_cars > region1_cars else "smooth"

            # Traffic light logic based on car count in regions
            if region1_cars > region2_cars:
                region1_light = "Green"
                region2_light = "Red"
                region1_text_color = (0, 255, 0)  # Green text for Region 1
                region2_text_color = (0, 0, 255)  # Red text for Region 2
            else:
                region1_light = "Red"
                region2_light = "Green"
                region1_text_color = (0, 0, 255)  # Red text for Region 1
                region2_text_color = (0, 255, 0)  # Green text for Region 2

            new_frame_time = time.time()
            fps = 1 / (new_frame_time - prev_frame_time)
            prev_frame_time = new_frame_time
            fps = int(fps)

            # Drawing updated information with traffic lights
            draw_text_with_background(annotated_frame, f'FPS: {fps}', (7, 30), bg_color=(0, 0, 0), text_color=(0, 255, 0))
            draw_text_with_background(annotated_frame, f'Total Cars: {len(track_ids)}', (7, 65), bg_color=(0, 0, 0), text_color=(0, 255, 0))
            draw_text_with_background(annotated_frame, f'Left Street Cars: {region1_cars}', (7, 105), bg_color=(255, 0, 0), text_color=(255, 255, 255))

            draw_text_with_background(annotated_frame, f'Road Condition: {region1_status}',
                                      (7, 139), bg_color=(255, 0, 0), text_color=region1_text_color)
            draw_text_with_background(annotated_frame, f'Traffic Light: {region1_light}',
                                      (7, 170), bg_color=(255, 0, 0), text_color=region1_text_color)

            draw_text_with_background(annotated_frame, f'Right Street Cars: {region2_cars}', (880, 105), bg_color=(0, 255, 0), text_color=(255, 255, 255))
            draw_text_with_background(annotated_frame, f'Road Condition: {region2_status}',
                                      (880, 139), bg_color=(0, 255, 0), text_color=region2_text_color)
            draw_text_with_background(annotated_frame, f'Traffic Light: {region2_light}',
                                      (880, 170), bg_color=(0, 255, 0), text_color=region2_text_color)

            out.write(annotated_frame)

        cap.release()
        out.release()
        cv2.destroyAllWindows()

output_path = "path/to/save/output_video.mp4"
video_processor = YOLOVideoProcessor(
    model_path="/content/runs/detect/train/weights/best.pt",  # Best YOLO model for vehicle detection
    video_path="/content/top-view-vehicle-detection-image-dataset/Vehicle_Detection_Image_Dataset/sample_video.mp4",  # Input video path
    output_path="/content/runs/detect/predict/tracking_result.avi",  # Output video path
    tracker_method="byte",  # Tracker method (BYTE)
    classes=[0]  # Vehicle class only
)
video_processor.process_video()

subprocess.run(['ffmpeg', '-y', '-loglevel', 'panic', '-i', "/content/runs/detect/predict/tracking_result.avi", 'tracking_result1.mp4'])

Video(r"/content/tracking_result1.mp4" , width=700 , height=500 , embed=True)

!pip install streamlit

